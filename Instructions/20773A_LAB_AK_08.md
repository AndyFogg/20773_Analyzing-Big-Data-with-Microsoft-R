# Module 8: Processing Big Data in SQL Server and Hadoop
# Lab A: Deploying a predictive model to SQL Server

## Exercise 1: Upload the flight delay data

#### Task 1: Configure SQL Server and create the FlightDelays database
01.  Log on to the **LON-DEV** VM as **Adatum\\AdatumAdmin** with the password **Pa55w.rd**.
02.  On the Windows desktop, click **Start**, type **Microsoft SQL Server Management Studio**, and then press Enter.
03.  In the **Connect to Server** dialog box, log on to **LON-SQLR** using Windows authentication.
04.  In the toolbar, click **New Query**.
05.  In the **Query** window, type the following commands. These commands enable you to run the **sp\_execute\_external\_script** stored procedure:
    ```
    sp_configure 'external scripts enabled', 1;
    RECONFIGURE;
    ```
06.  In the toolbar, click **Execute**.
07.  In **Object Explorer**, right-click **LON-SQLR**, and then click **Restart**.
08.  In the **Microsoft SQL Server Management Studio** message box, click **Yes**.
09.  In the second **Microsoft SQL Server Management Studio** message box, click **Yes**.
10.  Wait for SQL Server to restart.
11.  In **Object Explorer**, expand **LON-SQLR**, right-click **Databases**, and then click **New Database**.
12.  In the **New Database** dialog box, in the **Database name** text box, type **FlightDelays**, and then click **OK**.
13.  Leave SQL Server Management Studio open.

#### Task 2: Upload the flight delay data to SQL Server
1.  Open a command prompt window.
2.  Run the following commands:
    ```
    net use \\LON-RSVR\Data
    copy E:\Labfiles\Lab08\FlightDelayDataSample.xdf \\LON-RSVR\Data
    ```
3.  Verify that the file is copied successfully, and then close the command prompt.
4.  Start your R development environment of choice (Visual Studio, or RStudio), and create a new R file.
5.  In the script editor, add the following statement to the R file and run it. This code ensures that you are running in the local compute context:
    ```
    rxSetComputeContext(RxLocalSeq())
    ```
6.  In the script editor, add the following statement to the R file and run it. This code creates a connection string for the SQL Server **FlightDelays** database, and an **RxSqlServerData** data source for the **flightdelaydata** table in the database:
    ```
    connStr <- "Driver=SQL Server;Server=LON-SQLR;Database=FlightDelays;Trusted_Connection=Yes"
    flightDelayDataTable <- RxSqlServerData(connectionString = connStr,
	table = "flightdelaydata")
    ```
7.  Add the following code to the R file and run it. These statements import the data from the **FlightDelayDataSample**.xdf file and adds the **DelayedByWeather** logical factor and **Dataset** column to each observation:
    ```
    rxOptions("reportProgress" = 2)
    flightDelayDataFile <- "\\\\LON-RSVR\\Data\\FlightDelayDataSample.xdf"
    flightDelayData <- rxDataStep(inData = flightDelayDataFile,
    outFile = flightDelayDataTable, overwrite = TRUE,
    transforms = list(DelayedByWeather = factor(ifelse(is.na(WeatherDelay), 0, WeatherDelay) > 0, levels = c(FALSE, TRUE)),
    Dataset = factor(ifelse(runif(.rxNumRows) >= 0.05, "train", "test")))
    )
    ```

#### Task 3: Examine the data in the database
1.  Add the following statements to the R file and run it. This code creates a new SQL Server compute context:
    ```
    sqlWait <- TRUE
    sqlConsoleOutput <- TRUE
    sqlContext <- RxInSqlServer(connectionString = connStr,
    wait = sqlWait, consoleOutput = sqlConsoleOutput
    )
    rxSetComputeContext(sqlContext)
    ```
2.  Add the following code to the R file and run it. These statements create an **RxSqlServerData** data source that reads the flight delay data from the SQL Server database and refactors it:
    ```
    weatherDelayQuery = "SELECT Month, MonthName, OriginState, DestState,
    Dataset, DelayedByWeather, WeatherDelayCategory = CASE
    WHEN CEILING(WeatherDelay) <= 0 THEN 'No delay'
    WHEN CEILING(WeatherDelay) BETWEEN 1 AND 30 THEN '1-30 minutes'
    WHEN CEILING(WeatherDelay) BETWEEN 31 AND 60 THEN '31-60 minutes'
    WHEN CEILING(WeatherDelay) BETWEEN 61 AND 120 THEN '61-120 minutes'
    WHEN CEILING(WeatherDelay) BETWEEN 121 AND 180 THEN '121-180 minutes'
    WHEN CEILING(WeatherDelay) >= 181 THEN 'More than 180 minutes'
    END
    FROM flightdelaydata"
    delayDataSource <- RxSqlServerData(sqlQuery = weatherDelayQuery,
    colClasses = c(
    Month = "factor",
    OriginState = "factor",
    DestState = "factor",
    Dataset = "character",
    DelayedByWeather = "factor",
    WeatherDelayCategory = "factor"
    ),
    colInfo = list(
    MonthName = list(
    type = "factor",
    levels = month.name
    )
    ),
    connectionString = connStr
    )
    ```
3.  Add the following statement to the R file and run it. This statement retrieves the details for each variable in the data source:
    ```
	rxGetVarInfo(delayDataSource)
    ```
	There should be seven variables, named **Month**, **MonthName**, **OriginState**, **DestState**, **Dataset**, **DelayedByWeather**, and **WeatherDelayCategory**.

4.  Add the following statement to the R file and run it. This statement retrieves the data from the data source and summarizes it:
    ```
	rxSummary(~., delayDataSource)
    ```
5.  Add the following code to the R file and run it. This statement creates a histogram that shows the categorized delays by month:
    ```
    rxHistogram(~WeatherDelayCategory | MonthName,
    data = delayDataSource,
    xTitle = "Weather Delay",
    scales = (list(
    x = list(rot = 90)
    ))
    )
    ```
6.  Add the following code to the R file and run it. This statement creates a histogram that shows the categorized delays by origin state:
    ```
    rxHistogram(~WeatherDelayCategory | OriginState,
    data = delayDataSource,
    xTitle = "Weather Delay",
    scales = (list(
    x = list(rot = 90, cex = 0.5)
    ))
    )
    ```

	**Results**: At the end of this exercise, you will have imported the flight delay data to SQL Server and used ScaleR functions to examine this data.

## Exercise 2: Fit a DForest model to the weather delay data

#### Task 1: Create a DForest model
1.  Add the following code to the R file and run it. This statement fits a DTree model to the weather delay data:
    ```
    weatherDelayModel <- rxDForest(DelayedByWeather ~ Month + OriginState + DestState,
    data = delayDataSource,
    cp = 0.0001,
    rowSelection = (Dataset == "train")
    )
    ```
2.  Add the following statement to the R file and run it. This statement summarizes the forecast accuracy of the model:
    ```
	print(weatherDelayModel)
    ```
3.  Add the following statement to the R file and run it. This statement shows the structure of the decision trees in the DForest model:
    ```
	head(weatherDelayModel)
    ```
4.  Add the following statement to the R file and run it. This statement shows the relative importance of each predictor variable to the decisions made by the model:
    ```
	rxVarUsed (weatherDelayModel)
    ```

#### Task 2: Score the DForest model
1.  Add the following code to the R file and run it. This statement modifies the query used by the data source and adds a WHERE clause to limit the rows retrieved to the test data:
    ```
	delayDataSource@sqlQuery <- paste(delayDataSource@sqlQuery, "WHERE Dataset = 'test'", sep = " ")
    ```
2.  Add the following code to the R file and run it. This statement creates a data source that will be used to store scored results in the database:
    ```
	weatherDelayScoredResults <- RxSqlServerData(connectionString = connStr,
	table = "scoredresults")
    ```
3.  Add the following code to the R file and run it. These statements switch to the local compute context, generate weather delay predictions using the new data set and save the scored results in the **scoredresults** table in the database, and then return to the SQL Server compute context:
    ```
    rxSetComputeContext(RxLocalSeq())
    rxPredict(modelObj = weatherDelayModel,
    data = delayDataSource,
    outData = weatherDelayScoredResults, overwrite = TRUE,
    writeModelVars = TRUE,
    predVarNames = c("PredictedDelay", "PredictedNoDelay", "PredictedDelayedByWeather"),
    type = "prob")
    rxSetComputeContext(sqlContext)
    ```
4.  Add the following code to the R file and run it. This code tests the scored results against the real data and plots the accuracy of the predictions:
    ```
    install.packages('ROCR')
    library(ROCR)
    # Transform the prediction data into a standardized form
    results <- rxImport(weatherDelayScoredResults)
    weatherDelayPredictions <- prediction(results$PredictedDelay, results$DelayedByWeather)
    # Plot the ROC curve of the predictions
    rocCurve <- performance(weatherDelayPredictions, measure = "tpr", x.measure = "fpr")
    plot(rocCurve)
    ```
5.  If the **Microsoft Visual Studio** dialog boxes appear, click **Yes**.

**Results**: At the end of this exercise, you will have created a decision tree forest using the weather data held in the SQL Server database, scored it, and stored the results back in the database.

## Exercise 3: Store the model in SQL Server

#### Task 1: Save the model to the database
1.  Add the following code to the R file and run it. These statements created a serialized representation of the model as a string of binary data:
    ```
	serializedModel <- serialize(weatherDelayModel, NULL)
	serializedModelString <- paste(serializedModel, collapse = "")
    ```
2.  Return to SQL Server Management Studio.

3.  In the toolbar, click **New Query**. In the Query window, type the following code:
    ```
    USE FlightDelays;
    CREATE TABLE [dbo].[delaymodels]
    (
    modelId INT IDENTITY(1,1) NOT NULL Primary KEY,
    model VARBINARY(MAX) NOT NULL
    );
    ```
4.  In the toolbar, click **Execute**. Verify that the code runs without any errors.
5.  Overwrite the code in the **Query** window with the following block of Transact-SQL:
    ```
    CREATE PROCEDURE [dbo].[PersistModel] @m NVARCHAR(MAX)
    AS
    BEGIN
    SET NOCOUNT ON;
    INSERT INTO delaymodels(model) VALUES (CONVERT(VARBINARY(MAX),@m,2))
    END;
    ```
6.  In the toolbar, click **Execute**. Verify that the code runs without any errors.
7.  Return to your R development environment.
8.  Add the following statements to the R file and run it. This code uses an ODBC connection to run the **PersistModel** stored procedure and save your DTree model to the database:
    ```
    install.packages('RODBC')
    library(RODBC)
    connection <- odbcDriverConnect(connStr)
    cmd <- paste("EXEC PersistModel @m='", serializedModelString, "'", sep = "")
    sqlQuery(connection, cmd)
    ```

#### Task 2: Create a stored procedure that runs the model to make predictions
1.  Switch back to SQL Server Management Studio.
2.  Overwrite the code in the **Query** window with the following block of Transact-SQL:
    ```
    CREATE PROCEDURE [dbo].[PredictWeatherDelay]
    @Month integer = 1,
    @OriginState char(2),
    @DestState char(2)
    AS
    BEGIN
    DECLARE @weatherDelayModel varbinary(max) = (SELECT TOP 1 model FROM dbo.delaymodels)
    EXEC sp_execute_external_script @language = N'R',
    @script = N'
    delayParams <- data.frame(Month = month, OriginState = originState, DestState = destState)
    delayModel <- unserialize(as.raw(model))
    OutputDataSet<-rxPredict(modelObject = delayModel,
    data = delayParams,
    outData = NULL,
    predVarNames = c("PredictedDelay", "PredictedNoDelay", "PredictedDelayedByWeather"),
    type = "prob",
    writeModelVars = TRUE)',
    @params = N'@model varbinary(max),
    @month integer,
    @originState char(2),
    @destState char(2)',
    @model = @weatherDelayModel,
    @month = @Month,
    @originState = @OriginState,
    @destState = @DestState
    WITH RESULT SETS (([PredictedDelay] float, [PredictedNoDelay] float, [PredictedDelayedByWeather] bit, [Month] integer, [OriginState] char(2), [DestState] char(2)));
    END
    ```
3.  In the toolbar, click **Execute**. Verify that the code runs without any errors.
4.  Return to your R development environment.
5.  Add the following statements to the R file and run it. This code tests the stored procedure:
    ```
    cmd <- "EXEC [dbo].[PredictWeatherDelay] @Month = 10, @OriginState = 'MI', @DestState = 'NY'"
    sqlQuery(connection, cmd)
    ```
6.  Save the script as **Lab8\_1Script.R** in the **E:\\Labfiles\\Lab08** folder, and close your R development environment.
7.  Close SQL Server Management Studio, without saving any changes.

**Results**: At the end of this exercise, you will have saved the DForest model to SQL Server, and created a stored procedure that you can use to make weather delay predictions using this model.

# Lab B: Incorporating Hadoop Map/Reduce and Spark functionality into the ScaleR workflow

## Exercise 1: Using Pig with ScaleR functions

#### Task 1: Examine the Pig script
1.  Log on to the **LON-DEV** VM as **Adatum\\AdatumAdmin** with the password **Pa55w.rd**.
2.  Using **WordPad**, open the file **carrierDelays.pig** in the **E:\\Labfiles\\Lab08** folder.
3.  Review the script.
4.  Edit the first line of the script, and change the text **{specify login name}** to your login name, as shown in the following example:
    ```
	%declare loginName 'student01'
    ```
5.  Save the file and close Wordpad.

#### Task 2: Upload the Pig script and flight delay data to HDFS
1.  Start your R development environment of choice (Visual Studio, or RStudio), and create a new R file.
2.  In the script editor, add the following statements to the R file and run them. Change **student*nn*** to your login name, and then and run the code. These statements establish a new **RxHadoopMR** compute context:
    ```
    loginName = "studentnn"
    context <- RxHadoopMR(sshUsername = loginName,
    sshHostname = "LON-HADOOP",
    consoleOutput = TRUE)
    rxSetComputeContext(context, wait = TRUE)
    ```
3.  Add the following statements to the R file and run them. This code removes the **FlightDelayDataSample.csv** file from your directory in HDFS (if it exists; you can ignore the error message if it is not found), and copies the latest data from the E:\\Labfiles\\Lab08 folder:
    ```
    rxHadoopRemove(path = paste("/user/RevoShare/", loginName, "/FlightDelayDataSample.csv", sep="") )
    rxHadoopCopyFromClient(source = "E:\\Labfiles\\Lab08\\FlightDelayDataSample.csv",
    hdfsDest = paste("/user/RevoShare/", loginName, sep=""))
    ```
4.  Add the following statements to the R file and run them. This code uploads the **carriers.csv** file to your directory in HDFS:
    ```
    rxHadoopRemove(path = paste("/user/RevoShare/", loginName, "/carriers.csv", sep=""))
    rxHadoopCopyFromClient(source = "E:\\Labfiles\\Lab08\\carriers.csv",
    hdfsDest = paste("/user/RevoShare/", loginName, sep=""))
    ```
5.  Add the following statements to the R file and run them. Replace the text ***fqdn*** with the URL of the Hadoop VM in Azure (for example, LON-HADOOP-01.ukwest.cloudapp.azure.com). This code closes the **RxHadoopMR** compute context, creates a remote R session on the Hadoop VM, and loads the **RevoScaleR** library in that session:
    ```
	rxSetComputeContext(RxLocalSeq())
	remoteLogin(deployr_endpoint = "http://fqdn:12800", session = TRUE, diff = TRUE, commandline = TRUE, username = "admin", password = "Pa55w.rd")
	library(RevoScaleR)
    ```
6.  Add the following statements to the R file and run them. This code copies the Pig script (and the **loginName** variable) to the remote session:
    ```
    pause()
    putLocalFile("E:\\Labfiles\\Lab08\\carrierDelays.pig")
    putLocalObject(c("loginName"))
    resume()
    ```

#### Task 3: Run the Pig script and examine the results
1.  Add the following statement to the R file and run it. This code runs the Pig script:
    ```
	result <- system("pig carrierDelays.pig", intern = TRUE)
    ```
2.  Add the following code to the R file and run it. This code lists the contents of the /user/RevoShare/*studentnn* directory in HDFS. This directory should include a subdirectory named **results**:
    ```
	rxHadoopCommand(paste("fs -ls -R /user/RevoShare/", loginName, sep = ""), intern = TRUE)
    ```
3.  Verify that the results directory contains two files: **\_SUCCESS** and **part-r-00000**. The data is actually in the part-r-00000 file.
4.  Add the following statement to the R file and run it. This code deletes the **\_SUCCESS** file from the **results** directory:
    ```
	rxHadoopRemove(paste("fs -ls -R /user/RevoShare/", loginName, "/results/_SUCCESS", sep = ""))
    ```
5.  Add the following statements to the R file and run them. These statements display the structure of the results generated by the Pig script:
    ```
    rxOptions(reportProgress = 1)
    rxSetFileSystem(RxHdfsFileSystem())
    resultsFile <- paste("/user/RevoShare/", loginName, "/results", sep = "")
    resultsData <- RxTextData(resultsFile)
    rxGetVarInfo(resultsData)
    ```

#### Task 4: Convert the results to XDF format and add field names
1.  Add the following code to the R file and run it:
    ```
    carrierDelayColInfo <- list(V1 = list(type = "factor", newName = "Origin"),
    V2 = list(type = "factor", newName = "Dest"),
    V3 = list(type = "factor", newName = "AirlineCode"),
    V4 = list(type = "character", newName = "AirlineName"),
    V5 = list(type = "numeric", newName = "CarrierDelay"),
    V6 = list(type = "numeric", newName = "LateAircraftDelay")
    )
    ```
2.  Add the following code to the R file and run it. This code creates the **CarrierData** composite XDF file in HDFS:
    ```
    carrierFile <- RxXdfData(paste("/user/RevoShare", loginName, "CarrierData", sep = "/"))
    carrierData <- rxImport(inData = resultsData,
    outFile = carrierFile, overwrite = TRUE,
    colInfo = carrierDelayColInfo,
    createCompositeSet = TRUE)
    ```
3.  Add the following code to the R file and run it. This code creates a new **RxHadoopMR** compute context in the remote session:
    ```
	hadoopContext = RxHadoopMR(sshUsername = loginName, consoleOutput = TRUE)
	rxSetComputeContext(hadoopContext)
    ```
4.  Add the following code to the R file and run it. This code displays the structure of the XDF file which should now include the new mappings:
    ```
	rxGetVarInfo(carrierData)
    ```
5.  Add the following code to the R file and run it. This code summarizes the contents of the XDF file:
    ```
	rxSummary(~., carrierData)
    ```
Note that this function runs as a Map/Reduce job

#### Task 5: Create graphs to visualize the airline delay data
1.  Add the following code to the R file and run it. This code creates a data frame that contains the airline delay information (carrier delay plus late aircraft delay), and then uses this data frame to generate a histogram of delay times:
    ```
    transformedCarrierData <- rxDataStep(carrierData, transforms = list(TotalDelay = CarrierDelay + LateAircraftDelay))
    # The data source for this histogram is an in-memory data frame, so the processing is not distributed
    rxHistogram(~TotalDelay, data = transformedCarrierData,
    xTitle = "Carrier + Late Aircraft Delay (minutes)",
    yTitle = "Occurrences",
    endVal = 300
    )
    ```
2.  Add the following code to the R file and run it. This code changes the resolution of the plot window to 1024 by 768 pixels, and then generates a histogram showing the number of delayed flights for each airline:
    ```
    png(width=1024, height=768);rxHistogram(~AirlineCode, data = carrierData,
    xTitle = "Airline",
    yTitle = "Number of delayed flights"
    );dev.off()
    ```
3.  Add the following code to the R file and run it. This code creates a bar chart showing the total delay time for all flights made by each airline:
    ```
    library(ggplot2)
    ggplot(data = rxImport(carrierData, transforms = list(TotalDelay = CarrierDelay + LateAircraftDelay))) +
    geom_bar(mapping = aes(x = AirlineName, y = TotalDelay), stat = "identity") +
    labs(x = "Airline", y = "Total Carrier + Late Aircraft Delay (minutes)") +
    scale_x_discrete(labels = function(x) { lapply(strwrap(x, width = 25, simplify = FALSE), paste, collapse = "\n")}) +
    theme(axis.text.x = element_text(angle = 90, size = 8))
    ```

#### Task 6: Investigate the mean airline delay by route
1.  Add the following code to the R script and run it. This code creates a data cube that calculates the mean delay for each airline by route:
    ```
    delayData <- rxCube(AverageDelay ~ AirlineCode:Origin:Dest, carrierData,
    transforms = list(AverageDelay = CarrierDelay + LateAircraftDelay),
    means = TRUE,
    na.rm = TRUE, removeZeroCounts = TRUE)
    ```
2.  Add the following code to the R script and run it. This code uses the **rxExec** function to run **rxSort** to sort the data in the cube:
    ```
	sortedDelayData <- rxExec(FUN=rxSort, as.data.frame(delayData), sortByVars = c("Counts", "AverageDelay"), decreasing = c(TRUE, TRUE), timesToRun = 1)
    ```
3.  Add the following code to the R script and run it. This the top 50 rows in the sorted cube (the routes with the most frequent and longest delays).
    ```
	head(sortedDelayData$rxElem1, 50)
    ```
4.  Add the following code to the R script and run it. This code shows the bottom 50 rows in the sorted cube (the routes with the least frequent and shortest delays):
    ```
	tail(sortedDelayData$rxElem1, 50)
    ```
5.  Add the following code to the R script and run it. This code switches to the local compute context and saves the data cube to the file SortedDelayData.csv in HDFS:
    ```
    rxSetComputeContext(RxLocalSeq())
    sortedDelayDataFile <- paste("/user/RevoShare/", loginName, "/SortedDelayData.csv", sep = "")
    sortedDelayDataCsv <- RxTextData(sortedDelayDataFile)
    sortedDelayDataSet <- rxDataStep(inData = sortedDelayData$rxElem1,
    outFile = sortedDelayDataCsv, overwrite = TRUE
    )
    ```

**Results**: At the end of this exercise, you will have run a Pig script from R, and analyzed the data that the script produces.

## Exercise 2: Integrating ScaleR code with Spark and Hive

#### Task 1: Upload the sorted delay data to Hive
1.  Add the following code to the R script and run it. This code creates an **RxSpark** compute context:
    ```
    sparkContext = RxSpark(sshUsername = loginName,
    consoleOutput = TRUE,
    numExecutors = 10,
    executorCores = 2,
    executorMem = "1g",
    driverMem = "1g")
    rxSetComputeContext(sparkContext)
    ```
2.  Add the following code to the R script and run it. This code creates an **RxHiveData** data source:
    ```
	dbTable <- paste(loginName, "RouteDelays", sep = "")
	hiveDataSource <- RxHiveData(table = dbTable)
    ```
3.  Add the following code to the R script and run it. This code uploads the data to Hive:
    ```
	data <- rxDataStep(inData = sortedDelayDataSet,
	outFile = hiveDataSource, overwrite = TRUE)
    ```
4.  Add the following code to the R script and run it. This code runs the **rxSummary** function over the Hive data:
    ```
	rxSummary(~., hiveDataSource)
    ```

#### Task 2: Use Hive to perform ad-hoc queries over the data
1.  Open a command prompt window, and run the **putty** command.
2.  In the **PuTTY Configuration** window, select the **LON-HADOOP** session, click **Load**, and then click **Open**. You should be logged in to the Hadoop VM.
3.  In the PuTTY terminal window, run the following command:
    ```
	hive
    ```
4.  At the **hive\>** prompt, run the following command:
    ```
	select * from *studentnn*routedelays;
    ```
Replace *studentnn* with your login name.
Verify that 8852 rows are retrieved.
5.  At the **hive\>** prompt, run the following command:
    ```
	select count(*), airlinecode from *studentnn*routedelays group by airlinecode;
    ```
Replace *studentnn* with your login name.
This command lists each airline together with the number of delayed flights for that airline.
6.  At the **hive\>** prompt, run the following command to exit hive:
    ```
	exit;
    ```
7.  Close the PuTTY terminal window, and return to your R development environment.

#### Task 3: Use a sparklyr session to analyze data
1.  Add the following code to the R script and run it. This code loads the **sparklyr** and **dplyr** libraries.
    ```
	library(sparklyr)
	library(dplyr)
    ```
2.  Add the following code to the R script and run it. This code closes the current **RxSpark** compute context and creates a new one that supports sparklyr interop:
    ```
    rxSparkDisconnect(sparkContext)
    connection = rxSparkConnect(sshUsername = loginName,
    consoleOutput = TRUE,
    numExecutors = 10,
    executorCores = 2,
    executorMem = "1g",
    driverMem = "1g",
    interop = "sparklyr")
    ```
3.  Add the following code to the R script and run it. This code creates a new sparklyr session:
    ```
	sparklyrSession <- rxGetSparklyrConnection(connection)
    ```
4.  Add the following code to the R script and run it. This code lists the tables available in Hive:
    ```
	src_tbls(sparklyrSession)
    ```
5.  Add the following code to the R script and run it. This code caches your **routedelays** table and fetches the data in this table:
    ```
	tbl_cache(sparklyrSession, dbTable)
	routeDelaysTable <- tbl(sparklyrSession, dbTable)
	head(routeDelaysTable)
    ```
6.  Add the following code to the R script and run it. This code constructs a **dplyr** pipeline that finds the delays for all flights for American Airlines that departed from New York JFK, and saves the results in a tibble:
    ```
    routeDelaysTable %>%
    filter(AirlineCode == "AA" & Origin == "JFK") %>%
    select(Dest, AverageDelay) %>%
    collect ->
    aajfkData
    ```
7.  Add the following code to the R script and run it. This code displays the data in the tibble and summarizes it:
    ```
	print(aajfkData)
	rxSummary(~., aajfkData)
    ```
8.  Add the following code to the R script and run it. This code closes the sparklyr session and disconnects from the RxSpark compute context:
    ```
	rxSparkDisconnect(connection)
    ```
9.  Save the script as **Lab8\_2Script.R** in the **E:\\Labfiles\\Lab08** folder, and close your R development environment.

**Results**: At the end of this exercise, you will have used R code running in an RxSpark compute context to upload data to Hive, and then analyzed the data by using a sparklyr session running in the RxSpark compute context.

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
